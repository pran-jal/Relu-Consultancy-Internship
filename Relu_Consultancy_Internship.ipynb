{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Relu-Consultancy-Internship.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gyi4Q6QAJYS1"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "import requests as r\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!apt install -y xvfb\n",
        "\n",
        "!pip install undetected-chromedriver\n",
        "!pip install PyVirtualDisplay"
      ],
      "metadata": {
        "id": "nzWyTiYTJdP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fcd81d4-388b-478c-f085-2269b6350431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com] [1 InRelease 14.2 kB/88.7 kB 16%] [Connec\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 2s (101 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (101.0.4951.64-0ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 63 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 63 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: undetected-chromedriver in /usr/local/lib/python3.7/dist-packages (3.1.5.post4)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.7/dist-packages (from undetected-chromedriver) (10.3)\n",
            "Requirement already satisfied: selenium>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from undetected-chromedriver) (4.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from undetected-chromedriver) (2.28.1)\n",
            "Requirement already satisfied: urllib3[secure,socks]~=1.26 in /usr/local/lib/python3.7/dist-packages (from selenium>=4.0.0->undetected-chromedriver) (1.26.10)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.7/dist-packages (from selenium>=4.0.0->undetected-chromedriver) (0.9.2)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.7/dist-packages (from selenium>=4.0.0->undetected-chromedriver) (0.21.0)\n",
            "Requirement already satisfied: async-generator>=1.9 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium>=4.0.0->undetected-chromedriver) (1.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium>=4.0.0->undetected-chromedriver) (1.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium>=4.0.0->undetected-chromedriver) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium>=4.0.0->undetected-chromedriver) (2.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium>=4.0.0->undetected-chromedriver) (1.2.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium>=4.0.0->undetected-chromedriver) (21.4.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.7/dist-packages (from trio-websocket~=0.9->selenium>=4.0.0->undetected-chromedriver) (1.1.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium>=4.0.0->undetected-chromedriver) (1.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium>=4.0.0->undetected-chromedriver) (2022.6.15)\n",
            "Requirement already satisfied: cryptography>=1.3.4 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium>=4.0.0->undetected-chromedriver) (37.0.4)\n",
            "Requirement already satisfied: pyOpenSSL>=0.14 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium>=4.0.0->undetected-chromedriver) (22.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium>=4.0.0->undetected-chromedriver) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium>=4.0.0->undetected-chromedriver) (2.21)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.0.0->undetected-chromedriver) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium>=4.0.0->undetected-chromedriver) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->undetected-chromedriver) (2.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyVirtualDisplay in /usr/local/lib/python3.7/dist-packages (3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import undetected_chromedriver as uc"
      ],
      "metadata": {
        "id": "iOp38u_QoNWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agents = [\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246',\n",
        "            # 'Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; en) AppleWebKit/534.1+ (KHTML, Like Gecko) Version/6.0.0.141 Mobile Safari/534.1+',\n",
        "            # 'Mozilla/5.0 (iPhone; U; CPU like Mac OS X; en) AppleWebKit/420+ (KHTML, like Gecko) Version/3.0 Mobile/1A543a Safari/419.3',\n",
        "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36',\n",
        "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9',\n",
        "            'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n",
        "            'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36',\n",
        "            # 'Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.3 (KHTML, like Gecko) Chrome/6.0.472.63 Safari/534.3',\n",
        "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36',\n",
        "            # 'Mozilla/5.0 (Linux; U; Android 0.5; en-us) AppleWebKit/522+ (KHTML, like Gecko) Safari/419.3',\n",
        "            # 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-GB; rv:1.8.1.6) Gecko/20070725 Firefox/2.0.0.6',\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0',\n",
        "            'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1',\n",
        "            # 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:67.0) Gecko/20100101 Firefox/67.0',\n",
        "            # 'Opera/9.00 (Windows NT 5.1; U; en)',\n",
        "        ]"
      ],
      "metadata": {
        "id": "XbW7Ok4noiYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "        'dnt': '1',\n",
        "        'upgrade-insecure-requests': '1',\n",
        "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "        'sec-fetch-site': 'same-origin',\n",
        "        'sec-fetch-mode': 'navigate',\n",
        "        'sec-fetch-user': '?1',\n",
        "        'sec-fetch-dest': 'document',\n",
        "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
        "}"
      ],
      "metadata": {
        "id": "SufFmyfFoweK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def a_price(url, asin, headers):        # scrape price\n",
        "    \"\"\" \n",
        "    As some products have multiple sellers to choose from.\n",
        "    there is no 1 price set.\n",
        "    this set the price of all the different sellers.\n",
        "    returns the list of all the price or None.\n",
        "    \"\"\"\n",
        "    price = None\n",
        "    if r.get(url).status_code == 200:\n",
        "        pass\n",
        "    else:\n",
        "        try:\n",
        "            headers['referer'] = r.head(url).headers['Location']\n",
        "        except:\n",
        "            traceback.print_exc()\n",
        "            print(r.head(url).headers)\n",
        "        price_offer_list = r.get(f'https://www.amazon.de/gp/product/ajax/ref=auto_load_aod?asin={asin}&pc=dp&experienceId=aodAjaxMain', headers=headers).content\n",
        "        soup = BeautifulSoup(price_offer_list, 'html.parser')\n",
        "        prices = soup.find_all(\"span\",{\"class\":\"a-offscreen\"})\n",
        "        price = [re.sub(r\"[ ]*\\\\u(.){4}[ ]*\", '', i.text.strip()) for i in prices]\n",
        "    return price"
      ],
      "metadata": {
        "id": "Xj3YoNu4oycU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape(page, url, headers):     # scrape all and calls price scraper\n",
        "    \"\"\" \n",
        "    gets all the details required from the source of the product page.\n",
        "    title, price, details, image url are scraped.\n",
        "    Also calls the a_price to get price from the list of multiple sellers if required.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "    \n",
        "    product_title = soup.find(\"span\", {'id': \"productTitle\"}).text.strip()\n",
        "\n",
        "    space = re.compile(\"[ \\n\\t]{2,}\")\n",
        "    try:\n",
        "        details = re.sub(space, '\\n', soup.find(\"table\", {\"id\": \"productDetails_techSpec_section_1\"}).text)\n",
        "    except:\n",
        "        details = re.sub(space, '\\n', soup.find(\"div\", {\"id\":\"detailBullets_feature_div\"}).text)\n",
        "    \n",
        "    details = details.split('\\n')\n",
        "    for i in details:\n",
        "        details[details.index(i)] = i.strip().strip('\\u200e')\n",
        "    i = 0\n",
        "    len_D = len(details)\n",
        "    detail = {}\n",
        "    while i<len_D:\n",
        "        while True and i<len_D:\n",
        "            if details[i] == '' or details[i] == ':':\n",
        "                del details[i]\n",
        "                len_D -= 1\n",
        "            else:\n",
        "                break\n",
        "        else:break\n",
        "        while True and i<len_D-1:\n",
        "            if details[i+1] == '' or details[i+1] == ':':\n",
        "                del details[i+1]\n",
        "                len_D -= 1\n",
        "            else:\n",
        "                break\n",
        "        else:break\n",
        "        detail[details[i]] = details[i+1]\n",
        "        i=i+2\n",
        "    \n",
        "    price = None\n",
        "    price_s = soup.find(\"div\", {\"id\":\"tmmSwatches\"})\n",
        "    if price_s is not None:\n",
        "        price = price_s.text\n",
        "    else:\n",
        "        price_f = soup.find(\"div\", {\"id\": \"corePriceDisplay_desktop_feature_div\"})\n",
        "        if price_f is not None:\n",
        "            price = price_f.find(\"span\", {\"class\": \"a-offscreen\"}).text\n",
        "        \n",
        "        else:\n",
        "            link_of_p = soup.find(\"span\",{\"data-action\":\"show-all-offers-display\"}).find(\"a\", {\"class\": \"a-button-text\"})\n",
        "            if link_of_p is not None:\n",
        "                url = url.split('/dp/')\n",
        "                link = url[0]+link_of_p['href']\n",
        "                price = a_price(link, url[1], headers)\n",
        "            else:\n",
        "                price_list = soup.find_all(\"span\", {\"class\":\"a-offscreen\"})\n",
        "                price = [re.sub(re.compile(\"[A-Za-z]\"), '', i.text.strip().strip('\\u20ac')) for i in price_list]\n",
        "\n",
        "    try:\n",
        "        image = soup.find(\"img\", {\"id\": \"landingImage\"})['src']\n",
        "    except:\n",
        "        image = soup.find(\"img\", {\"id\": \"imgBlkFront\"})['src']\n",
        "    \n",
        "    return {\n",
        "        \"title\":product_title,\n",
        "        \"image_url\": image,\n",
        "        \"price\": price,\n",
        "        \"details\": detail\n",
        "    }"
      ],
      "metadata": {
        "id": "L3kHI2Ieo9Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sel(url, headers):              # selenium in case scraper detected\n",
        "    \"\"\" \n",
        "    some pages require webdriver to get the source code of the product page.\n",
        "    loads the product in selenium to get the page source.\n",
        "    and then calls the scraper.\n",
        "    \"\"\"\n",
        "    browser = uc.Chrome()\n",
        "    options = uc.ChromeOptions()\n",
        "    options.headless = True\n",
        "    options.add_argument('headless')\n",
        "    browser.get(url)\n",
        "    time.sleep(1)\n",
        "    source = browser.page_source\n",
        "    browser.quit()\n",
        "    return scrape(source, url, headers)"
      ],
      "metadata": {
        "id": "tB0X5yOwo9yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\" \n",
        "    main function.\n",
        "    reads the csv file.\n",
        "    scrapes every url if available.\n",
        "    saves progress to json file every 100 url passed.\n",
        "\n",
        "    \"\"\"\n",
        "    f = open(\"Sheet1.csv\", 'r')\n",
        "    csv_file = csv.reader(f)\n",
        "\n",
        "    to_json = []\n",
        "    count = 0\n",
        "\n",
        "    begin = time.time()\n",
        "    print('scraping...')\n",
        "    for line in csv_file:\n",
        "        \n",
        "        count +=1\n",
        "        \n",
        "        asin = line[2]\n",
        "        country = line[3]\n",
        "        headers['referer'] = f\"https://www.amazon.{country}\"\n",
        "        headers['user-agent'] = random.choice(agents)\n",
        "        \n",
        "        url = f\"https://www.amazon.{country}/dp/{asin}\"\n",
        "        a = r.get(url, headers=headers)\n",
        "    \n",
        "        if a.status_code == 200:\n",
        "            try:\n",
        "                to_json.append(scrape(a.content, url, headers))\n",
        "            except:\n",
        "                # traceback.print_exc()\n",
        "                try:\n",
        "                    to_json.append(sel(url, headers))\n",
        "                except:\n",
        "                    to_json.append({\"Error\": f\"the {url} not available\"})\n",
        "        else:\n",
        "            to_json.append({\"Error\": f\"the {url} not available\"})\n",
        "        \n",
        "        if count == 100:\n",
        "            end = time.time()\n",
        "            print(int(line[0])-1, \" done -\", end - begin, \"seconds\")\n",
        "            begin = time.time()\n",
        "            count = 0\n",
        "            j = open('scraped.json', 'a', encoding='utf-8')\n",
        "            json.dump(to_json, j)\n",
        "            j.close()\n",
        "\n",
        "    end = time.time()\n",
        "    print(int(line[0])-1, \" done -\", end - begin, \"seconds\")\n",
        "    count = 0\n",
        "    j = open('scraped.json', 'a', encoding='utf-8')\n",
        "    json.dump(to_json, j)\n",
        "    j.close()"
      ],
      "metadata": {
        "id": "c-so_z07sVFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LedRksJFvx5j",
        "outputId": "cda2feef-00be-4acc-9da1-3e7a426a80b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Credicxo-Tech-Internship/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_1oCKdWv1lb",
        "outputId": "68b4ed2f-028e-47ec-83d2-b3917acdd4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Credicxo-Tech-Internship\n"
          ]
        }
      ]
    }
  ]
}